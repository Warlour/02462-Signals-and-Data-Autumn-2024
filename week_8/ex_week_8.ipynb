{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73d30a8",
   "metadata": {},
   "source": [
    "# NLP, N-grams and FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5e74f",
   "metadata": {},
   "source": [
    "As you have seen in the lectures, NLP has a wide range of techniques and applications of such techniques. We will give you an introduction to some of these techniques, and today you will get hands-on experience with them. In today's exercise, we will look at the following topics:\n",
    "\n",
    "1. How do we represent text in a vectorized way that encodes context? (One answer here is N-grams, and those we will look at).\n",
    "2. How do we create and sample from an N-gram language model - and how does the size of the grams affect the generated text?\n",
    "3. How do we use a pre-existing language model (FastText), to classify text messages as spam?\n",
    "\n",
    "The data we will be using later today is a dataset consisting of \"spam or ham\" text messages. The dataset consists of a number of text messages, some of which are spam and some of which are so-called \"ham\". We will use FastText to classify mails as spam or ham. For now, we will be looking at some different texts, to see how we can use N-grams to generate text, and how we can create N-gram language models from a text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f896138af3796",
   "metadata": {},
   "source": [
    "## Exercise 1: Text-loading\n",
    "\n",
    "\n",
    "The texts we will be experimenting with N-grams on are the two famous books Pride & Prejudice by Jane Austen and The Origin of Species by Charles Darwin. The two books have been obtained in a raw text format from https://www.gutenberg.org/, i.e. Project Gutenberg which concerns itself with the collection of Open Access e-books.\n",
    "\n",
    "A big part of working with text documents is unfortunately having to preprocess the documents. Preprocessing of these, can have a large impact on the eventual performance of language models, such as N-gram models. We have included the text-preprocessing steps in the cell below. In the output cell you will notice that the first chapter of pride and prejudice is printed out. It is then preprocessed using the `preprocess_text` function and printed out again.\n",
    "\n",
    "**1. Implement the pre_processing function according to the following description:**\n",
    "1. Remove empty characters using `.strip()`\n",
    "2. All text should be lowercase \n",
    "    - *Python strings have an in-built method for that*\n",
    "3. Remove all special characters by using the regex pattern `r\"[^a-zA-Z0-9.?! \\n]+\"` and the sub function from Python's re module\n",
    "4. Split the text by lines *\"\\n\" is the character used for newlines*\n",
    "5. Remove chapter headlines\n",
    "6. Recreate the full document again using `\"\\n\".join(text)`\n",
    "7. Replace `\"\\n\"` with `\" \"` and double-spacing with single spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f31431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import fasttext\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97712834817df01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = \n",
    "    #TODO: 8 lines of code\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3539556806549c",
   "metadata": {},
   "source": [
    "**2. The preprocessing is not perfect. Do you notice any issues in the text? Show examples of words that will may be problematic:**\n",
    "\n",
    "*HINT: What happens to *good-humoured*? What happens to *three-and-twenty*? What happens to *Mr.* and *Mrs.*, and how will this later be handled when we split the sentences?*\n",
    "\n",
    "If two words are separated by a character that we remove they will become a single word. And creating sentences through the use of full-stops means \"Mr. Harvey\" will become two sentences \"Mr\" and \"Harvey\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3f48806447a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocess_text(\"good-humoured\"))\n",
    "print(preprocess_text(\"Mr.\"))\n",
    "print(preprocess_text(?))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4b59a93a1934d",
   "metadata": {},
   "source": [
    "**3. Apply the preprocessing to Pride and Prejudice and the Origin of Species**\n",
    "\n",
    "*Hint: Use the `.read` method on the file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88535013892b3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pride_and_prejudice.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    pride_n_pred = ?\n",
    "    pride_n_pred_preproc = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pride_and_prejudice.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    orig_of_spec = ?\n",
    "    orig_of_spec_preproc = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81760d4c6a0483b2",
   "metadata": {},
   "source": [
    "## Exercise 2: Creating N-grams\n",
    "\n",
    "Now that we have the texts in the preprocessed document format we want, we will move forward with the creation of our N-grams. Recall we want to use the N-grams for probabilistic word modelling tasks, for example, next word predictions given some sequence of words which we can express the following way:\n",
    "\n",
    "$$\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1})\n",
    "$$\n",
    "The problem is that estimating such probabilities for very long sequences is computationally and memory-wise VERY expensive. So as a solution we sometimes use N-grams. In N-grams, the assumption is that we can model these conditional dependencies with shorter sequences of words, i.e.:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1}) & \\approx P(w_n) \\quad \\text{(Unigram)}\\\\\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1}) & \\approx P(w_n| w_{n-1}) \\quad \\text{(Bigram)}\\\\\n",
    "P(w_n|w_1, w_2, ..., w_{n-2}, w_{n-1}) & \\approx P(w_n|w_{n-2}, w_{n-1}) \\quad \\text{(Trigram)}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Which we then compute as:\n",
    "\n",
    "$$\n",
    "P(w_n|w_{n-2}, w_{n-1}) = \\frac{\\text{Count}(w_{n-2}, w_{n-1}, w_n)}{\\text{Count}(w_{n-2}, w_{n-1})}\n",
    "$$\n",
    "The language model that we create is based on some text corpus from which we obtain the count measures. In this exercise we will try making such N-gram models on the two books Origin of Species and Pride and Prejudice!\n",
    "\n",
    "In the cell below we have written the functions required for preprocessing a corpus even further such that it is ready for creating an N-gram model on. In order to guarantee that we can just start text generation or give conditional probabilites for how likely a start or end word is in given sentence we pad our sentence with start and end tokens denoted as `<s>` and `</s>` according to the size of N-grams we are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09451a7a9acfb3",
   "metadata": {},
   "source": [
    "**1. Why do N-grams encode context in comparison to methods such as count vectorizers which just count words?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e781c11ddabe00",
   "metadata": {},
   "source": [
    "**2. Implement the function `tokenize_and_pad` that takes a corpus and a parameter `N` specifying the amount of padding to apply and returns a list of lists where the inner lists are sentences split into words**\n",
    "1. Split the corpus into sentences *Using a full stop as the delimiter*\n",
    "2. Calculate the length of the padding needed\n",
    "    - Think about how many words are needed to create the first N-gram? *Hint: The first actual word is included*\n",
    "3. Create the padding for the front and end using for example `\" \".join(list_of_start_chars)`\n",
    "    - In Python you can create a list of length pad_len using `[el] * pad_len`\n",
    "4. Pad each corpus sentence ensuring that there is a space between the padding and sentence\n",
    "    - Use the .strip() method on each sentence to remove empty characters\n",
    "Split the padded sentences into words and return a list of lists containing the split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e289604555b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad(corpus, N):\n",
    "    corpus_sentences = \n",
    "    #TODO: 7-10 lines of code\n",
    "    return tokenized_corpus_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792e4ef5c318a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "orig_of_spec_tokenize = tokenize_and_pad(orig_of_spec_preproc, N=N)\n",
    "print(orig_of_spec_tokenize[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00d6b020a66d31",
   "metadata": {},
   "source": [
    "**3. Implement the function `create_n_grams` which takes the output from the above function and splits it into n_grams**\n",
    "1. Create and empty list for the n_grams\n",
    "2. Loop across all the tokenized sentences\n",
    "3. Now loop across the range of applicable starting indexes for the given sentence\n",
    "    - If N = 2, what's the length of the sentences list and how many N_grams can we create? Generalise this.\n",
    "4. Create the current n_gram by using `\" \".join`\n",
    "5. Return the n_grams list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641869505c82378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_grams(tokenized_sentences, N):\n",
    "    #TODO: 6 lines of code\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7cd4fa951b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_of_spec_n_grams = create_n_grams(orig_of_spec_tokenize, N=N)\n",
    "print(orig_of_spec_n_grams[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36ef10d8c6c6ef",
   "metadata": {},
   "source": [
    "**4. Implement the function `n_grams_to_prob_map` which takes a list of n_grams as returned by the function above and create a dictionary which maps the probabilities of words occurring after a given context.**\n",
    "1. If you are unfamiliar with the defaultdict class in Python, read this article before you start this task https://www.geeksforgeeks.org/defaultdict-in-python/\n",
    "2. If you are unfamiliar with the lambda keyword in Python, read this article before you start this task https://www.geeksforgeeks.org/python-lambda-anonymous-functions-filter-map-reduce/?ref=header_outind\n",
    "3. Create a default dict, name it `contexts`, with a default value that is a default dict whose default value is 0\n",
    "    - In essence: defaultdict(defaultdict(0))\n",
    "    - This will be used to count how often each target word occurs after a certain context\n",
    "4. Loop through every n-gram and split it\n",
    "5. Create the context by using `\" \".join` on all but the last token in the n-gram. The last token is the target\n",
    "6. Increment (means adding 1 to a value) the counter for the target value for the given context\n",
    "7. Create a new dictionary `cond_prob`\n",
    "8. Loop across the keys of `contexts`\n",
    "9. Create a list of each target in the current context by wrapping the .keys() call in a list()\n",
    "10. Count the number of occurrences of each target in the current context and put them in a numpy array\n",
    "11. Calculate the probabilities of each target by normalising the occurrence count \n",
    "    - Ensure it sums to 1\n",
    "12.  Assign a tuple of the targets and their probabilities to the entry in the `cond_prob` function\n",
    "    - The order of the tuple is important for later, so (context_targets, targets_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8a8b43a597e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_default_dict():\n",
    "    return ?\n",
    "\n",
    "def n_grams_to_prob_map(n_grams):\n",
    "    contexts = defaultdict(nested_default_dict)\n",
    "    #TODO: 12-15 lines of code\n",
    "    return cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63d602ce669b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_of_spec_cond_prob = n_grams_to_prob_map(orig_of_spec_n_grams)\n",
    "context_test = \" \".join(orig_of_spec_n_grams[60].split(\" \")[:-1])   # \"he is\" has a good number of targets\n",
    "targets, probs = orig_of_spec_cond_prob[context_test]\n",
    "\n",
    "sorted_targets, sorted_probs = zip(*sorted(zip(targets, probs), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sorted_targets, sorted_probs, color='skyblue')\n",
    "plt.xlabel(\"Target Words\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(f\"Conditional Probability Distribution for Context: '{context_test}'\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()  # Adjust layout for readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45101af022fd270e",
   "metadata": {},
   "source": [
    "**5. Vary the N-gram size N and inspect the first 20 N-grams.**\n",
    " - How do they change and why? \n",
    " - Do you think there could be issues with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9b9f9d2251179",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=3\n",
    "orig_of_spec_tokenize = tokenize_and_pad(orig_of_spec_preproc, N=N)\n",
    "orig_of_spec_n_grams = create_n_grams(orig_of_spec_tokenize, N=N)\n",
    "orig_of_spec_cond_prob = n_grams_to_prob_map(orig_of_spec_n_grams)\n",
    "print(orig_of_spec_n_grams[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e9e1a",
   "metadata": {},
   "source": [
    "## Exercise 3: Generating Text\n",
    "\n",
    "In the previous exercise we saw how to tokenize a corpus such that it is ready to be split into n-grams. We then saw how to make n-grams and create a conditional probability based on these.\n",
    "\n",
    "The question now is, how can we generate a text using this conditional probability. A way of doing this is to sample from a conditional probability distribution based on our obtained N-grams. In essence, we can give a seed to our conditional probability (also called a context), and then we need to generate a word from our conditional probability by sampling from it.\n",
    "\n",
    "In the code below we will define a function that allows us to generate a sentence based on a provided conditional distribution. In the cell we create such a conditional distribution and generate 5 sentences using the same text seed. \n",
    "\n",
    "**1. Implement the `generate_text` function.**\n",
    "1. Create a variable which will be the output string and assign the text_seed to it\n",
    "2. Split the text_seed into words and check its length, it should be N-1\n",
    "    - If the text seed is too short append the start character to it\n",
    "    - If the text seed is too long change it to the last N-1 words\n",
    "3. Create a variable that holds the current context (text_seed right now)\n",
    "4. Loop across the number of words we wish to generate\n",
    "   - If the current context is not in the `cond_prob` dictionary, return the generated sentence\n",
    "   - If it is, sample a target word from the context according to its probability distribution and update the context for the next iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90afe4279bf81ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(cond_prob, text_seed, N, num_words=25):\n",
    "    #TODO: 15-17 lines of code\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9a5faf0c5dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=2\n",
    "orig_of_spec_tokenize = tokenize_and_pad(orig_of_spec_preproc, N=N)\n",
    "orig_of_spec_n_grams = create_n_grams(orig_of_spec_tokenize, N=N)\n",
    "orig_of_spec_cond_prob = n_grams_to_prob_map(orig_of_spec_n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2339b75e29fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_seed = \"he is a\"\n",
    "for i in tqdm(range(5)):\n",
    "    print(generate_text(cond_prob=orig_of_spec_cond_prob, text_seed=text_seed, N=N) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81aca6ee7890985",
   "metadata": {},
   "source": [
    "**2. Answer the following questions about the text generation you have just implemented:**\n",
    "- Why is it that even though we use the same text seed, the generated sentences changes?\n",
    "\n",
    "\n",
    "- What happens as you increase the N-gram size as shown in the cell below? Does this makes sense - and if so, why?\n",
    "\n",
    "\n",
    "- Is it more optimal to have smaller or larger N-gram size? Try to experiment with generated sentences as N goes from 2->7.\n",
    "\n",
    "\n",
    "- What would it mean to set the N-gram size to one? What would you expect the generated text to look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59edaf21e94dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in tqdm(range(1, 8)):\n",
    "    orig_of_spec_tokenize = tokenize_and_pad(orig_of_spec_preproc, N=N)\n",
    "    orig_of_spec_n_grams = create_n_grams(orig_of_spec_tokenize, N=N)\n",
    "    orig_of_spec_cond_prob = n_grams_to_prob_map(orig_of_spec_n_grams)\n",
    "    print(f\"{N}: {len(orig_of_spec_cond_prob.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54164f68",
   "metadata": {},
   "source": [
    "**3. We will now look at how the generated sentences changes depending on the corpus used to create our n-grams on.**\n",
    "\n",
    "- Create N-grams and a conditional probability using the Pride and Prejudice corpus.\n",
    "- Try to generate some sentences using both conditional probabilites but using the same text seed (use ngram size 3 for example and use the provided text seed for both n-gram models. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here for creating a sentence generator using Pride and Prejudice as your corpus\n",
    "#and comparing the two language models.\n",
    "N=3\n",
    "pride_n_pred_tokenize = tokenize_and_pad(pride_n_pred_preproc, N=N)\n",
    "pride_n_pred_n_grams = create_n_grams(pride_n_pred_tokenize, N=N)\n",
    "pride_n_pred_cond_prob = n_grams_to_prob_map(pride_n_pred_n_grams)\n",
    "\n",
    "text_seed = \"it is said that\"\n",
    "\n",
    "for i in range(5):\n",
    "    print(generate_text(cond_prob=pride_n_pred_cond_prob, text_seed=text_seed, N=N)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e3289",
   "metadata": {},
   "source": [
    "# FastText for Ham or Spam\n",
    "\n",
    "In the following exercises we will be looking at classifying text messages as \"Ham\" or \"Spam\" by using Fasttext models. At first we will format our data to comply with the Fasttext library, then implement our own Pytorch model to run on the data and finally use the much faster Fasttext library to achieve the results. Remember that the coding is optional for this course and is mainly there for those who wish to get comfortable with Pytorch data loading (A highly valuable skill for your future as an ML engineer!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f29aa0",
   "metadata": {},
   "source": [
    "## Exercise 4: Loading spam or ham data\n",
    "\n",
    "In the following cells we use the pandas library to load our text delimited file which has the labels in the first column and the text messages in the second. Fasttext expects a file where each line has the signature  `__label__{label} text` (where `__label__` is a token, i.e. something the FastText library reads as a keyword)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd761cca85d3c",
   "metadata": {},
   "source": [
    "**1. Create a function that given a dataframe containing a column of texts and a column of labels, creates a .txt file where each line starts with `__label__x y` where `x` is the label and `y` is the text.**\n",
    "1. Iterate across the labels and text and write each line to a string in the correct format\n",
    "2. Use a `with open(path_to_doc)` clause to write the text to a document\n",
    "3. Include a print statement tha prints the distribution of labels in the test which is only executed if `verbose=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1024b9656df3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fasttext_format_txt(data_frame, path_to_doc, verbose=True):\n",
    "    texts = list(data_frame['1'])\n",
    "    labels = list(data_frame['0'])\n",
    "    txt = \"\"\n",
    "    \n",
    "    # Construct FastText formatted string\n",
    "    #TODO: 4 lines of code\n",
    "    # Optional verbose output for label distribution\n",
    "    if verbose:\n",
    "        #TODO: 4 lines of code\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e60d5f7eee85f",
   "metadata": {},
   "source": [
    "**2. Use the `pandas.read_csv` function to read the `SMS_train.txt` file. Look up the documentation by googling. It may also require you to inspect the text file.**\n",
    "- How many text messages are in the training set?\n",
    "- What's the distribution of labels and how will that impact our training of a classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004909b3d1703dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(\"data\", \"SMS_train.txt\"))\n",
    "test_data = pd.read_csv('./data/SMS_test.txt', delimiter=',', encoding=\"utf-8\")\n",
    "\n",
    "train_texts, train_labels = create_fasttext_format_txt(data_frame=train_data, path_to_doc='data/train_data.txt')\n",
    "test_texts, test_labels = create_fasttext_format_txt(data_frame=test_data, path_to_doc='data/test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560fa2dcd23c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f796458b5bf941",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a23288a7d9d97e",
   "metadata": {},
   "source": [
    "# Exercise 5: Building our own Fasttext model\n",
    "As you should know by now there are libraries for most things in Python and Fasttext is no Exception. Nonetheless, it is valuable to build the architecture from \"scratch\" to truly understand what is going on. \n",
    "We will use Pytorch, which means we have to create our own dataloader. Since we are working with text data that is embedding through a learned embedding, we will include the embedding part of the network in the Dataloader which is probably not best practice, but done for coding simplicity. \n",
    "\n",
    "The embedding module takes as input the indices of each word in a given vocabulary and outputs a vector of the specified dimension as output. This way the gradient can be backpropagated to the embedding by changing the embedding of a corresponding index based on the error. Ask a TA if you want a more in-depth explanation.\n",
    "\n",
    "Typical hyperparameters for Fasttext\n",
    "- Embedding dimension: 300\n",
    "- Batch size: low (1 - you'll help figure this out)\n",
    "- N_grams: 3-6\n",
    "- Learning rate: the classic $10^{-3}$ to $10^{-5}$\n",
    "\n",
    "**If you do not want to implement this by yourself, simply copy the dataloader from the solution and move on**\n",
    "\n",
    "The Dataloader has to accomplish the following:\n",
    "- Loading the txt file of the format created above\n",
    "- Processing each line of text\n",
    "    - Use the parts of `preprocess_text` that apply to a single sentence\n",
    "    - Create the word n-grams or character n-gram of each text using the two functions defined in the cells before the dataloader \n",
    "- Build the vocabulary, we must be able to pass the vocabulary from the train loader to the test loader\n",
    "- Count the distribution of classes\n",
    "- Create a new `nn.Embedding` module unless one is passed to the dataloader. For testing we need to use the same embedding as we did for training and thus we need to be able to re-use it in the Dataloader.\n",
    "\n",
    "\n",
    "**1. $\\star$ Implement the following functions to be used for processing a line of text in the Fasttext format and creating word and character grams** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7b7838c7f6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    \"\"\"Given a line assuming the '__label__{label} text' format, split it into the label and text, strip the text, make it lowercase\n",
    "    remove special characters and remove newlines and double spaces\"\"\"\n",
    "    label, text = line.split(maxsplit=1)\n",
    "    #TODO: 5 lines of code\n",
    "    return label, text\n",
    "\n",
    "with open('data/train_data.txt', \"r\") as f:\n",
    "    for line in f.readlines()[:10]:\n",
    "        print(process_line(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb8fff5fa360ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_grams_from_sentence(sentence, N):\n",
    "    \"\"\"Given a sentence and int N, pad the sentence with the start and end character and return a list of word N-grams\"\"\"\n",
    "    #TODO: 8-10 lines of code\n",
    "    return sentence_n_grams\n",
    "\n",
    "example_word_grams = create_word_grams_from_sentence(\"a simpel simple sentence\", 2,)\n",
    "print(example_word_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c88c7d396f02dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_grams_from_sentence(sentence, min_n, max_n):\n",
    "    \"\"\"Given a sentence and the range of character n-grams to produce, return a list of the character grams\"\"\"\n",
    "    assert 0 < min_n\n",
    "    assert 0 < max_n\n",
    "    assert min_n <= max_n\n",
    "    # We will use .extend to append all the elements from a list to another list\n",
    "    #TODO: 4 lines of code\n",
    "    return all_char_grams\n",
    "\n",
    "example_character_grams = create_char_grams_from_sentence(\"a simpel simple sentence\", 2, 3)\n",
    "print(example_character_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09d3eac76b3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    \"\"\"Given a list of texts, count the number of unique words and map each to an index. Add an unknown at the end (for safety)\"\"\"\n",
    "    # Look up the Counter class in python for getting the frequencies of each object\n",
    "    # This will be used to count the number of unique words\n",
    "    counter = Counter()\n",
    "    #TODO: 4 lines of code\n",
    "    return vocab\n",
    "build_vocab([example_word_grams for _ in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e839fe7fdb9e46",
   "metadata": {},
   "source": [
    "**2. $\\star \\star$ Using the description and functions, implement the dataset.**\n",
    "\n",
    "*The dataset is of a type [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html), usually used when we need a bit more functionality than just yielding items from a list or array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e50d4f681d5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, embed_dim=300, embedding=None, vocab=None, N_gram=3, minn=0, maxn=0, ):\n",
    "        self.data = []\n",
    "        self.labels = set()\n",
    "        self.class_counts_dict = defaultdict(lambda: 0)\n",
    "        self.N_gram = N_gram\n",
    "        self.minn = minn\n",
    "        self.maxn = maxn\n",
    "        print(f\"Using {'word' if self.minn == 0 or self.maxn == 0 else 'char'} gram model\")\n",
    "        self.process_file(file_path=file_path)\n",
    "        \n",
    "        # Build vocabulary and label-to-index mappings\n",
    "        self.vocab: dict = ? if vocab is None else vocab\n",
    "        self.vocab_size: int = ?\n",
    "        self.label_to_idx: dict = {label: idx for idx, label in enumerate(sorted(self.labels)[::-1])}\n",
    "        self.num_classes: int = ?\n",
    "        self.embedding = nn.Embedding(?) if embedding is None else embedding\n",
    "        sorted_labels: list = sorted(self.class_counts_dict.keys(), key=lambda label: self.label_to_idx[label])\n",
    "        # Sort class_counts based on the sorted order of labels\n",
    "        self.class_counts: torch.tensor = ?\n",
    "    \n",
    "    def process_file(self, file_path):\n",
    "        # Load and process data\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                label, text = ?\n",
    "                \n",
    "                if not text.strip():\n",
    "                    print(line.replace('\\n', '') + f\" = {text} was skipped\")\n",
    "                    continue\n",
    "                \n",
    "                if self.minn == 0 or self.maxn == 0:\n",
    "                    text = ?\n",
    "                else:\n",
    "                    text = ?\n",
    "                \n",
    "                self.labels.add(label)\n",
    "                self.data.append((text, label))\n",
    "                self.class_counts_dict[label] += 1\n",
    "\n",
    "    def text_embedded(self, text):\n",
    "        \"\"\" Convert each token to its corresponding index using the vocabulary.get() setting the default to the unknown token, convert to int using .int()\n",
    "        Then use the embedding module to embed the indices and take the mean across dim=0.\"\"\"\n",
    "        text_indices = ?\n",
    "        embedded = ?\n",
    "        if torch.isnan(embedded).any():\n",
    "            print(f\"Embedding contains nans!\")\n",
    "            print(text)\n",
    "        return embedded\n",
    "\n",
    "    def label_to_tensor(self, label):\n",
    "        \"\"\"Convert the label to its corresponding index, cast it to a tensor and call the .long() method on the tensor to ensure the correct dtype\"\"\"\n",
    "        return ?\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of datapoints in the dataset.\"\"\"\n",
    "        return ?\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return the text and label at the given index by converting the label to a tensor and the embedding the text.\"\"\"\n",
    "        text, label = ?\n",
    "        return ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b8d0e565825ad",
   "metadata": {},
   "source": [
    "The neural network for Fasttext is a single layer that maps the embedding dimension to the output dimension\n",
    "\n",
    "**3. Define this simple network and its forward function in the class below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3483dad9325d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        #TODO: 1 line of code\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255f8d1cd5c7ff7",
   "metadata": {},
   "source": [
    "**4. As we saw in the label distribution there is an overweight of the ham class in the data. Create a function which calculates the inverse class frequencies given a tensor of class counts**\n",
    "- The inverse class frequency for a given class is $ICF(c_1) = \\frac{\\sum_i^{N}count(c_i)}{count(c_1)}$, where $N$ is the number of classes and $count(x)$ counts the number of occurrences of a given class in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81d95f5a3c043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICF(class_counts):\n",
    "    return ?\n",
    "\n",
    "ICF(torch.tensor((3, 1)))   # Inputting (3, 1) should yield (4/3, 4/1) = (1.3333333, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17d5233477f843",
   "metadata": {},
   "source": [
    "**5.Train the network using the training loop below. Test different configurations of the batch size, embedding dimension and investigate whether using the class weights makes a difference**\n",
    "- We calculate sensitivity and specificity. Look at the calculations in code and explain what they each measure\n",
    "\n",
    "It calculates the proportion of each class which is classified correctly. In other words, in our case sensitivity is how well spam is detected and specificity how well ham is detected.\n",
    "- Start with 10 epochs, but if you see the network converge earlier you may reduce this number\n",
    "- Checklist of parameters to potentially vary:\n",
    "    - N gram size\n",
    "    - Using word vs character grams\n",
    "    - batch size\n",
    "    - embedding dimension\n",
    "    - class weights\n",
    "    - learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e921943c18c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "N_gram = 2\n",
    "minn = 0\n",
    "maxn = 0\n",
    "# If we specify BOTH minn and maxn, we use a character gram model\n",
    "dataset_train = TextDataset('data/train_data.txt', embed_dim=embed_dim, N_gram=N_gram, minn=minn, maxn=maxn)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "class_weights = ICF(dataset_train.class_counts)\n",
    "print(\"Class_weights:\", class_weights)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936fdc49dc920fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "model = TextClassifier(embed_dim=embed_dim, num_classes=dataset_train.num_classes)  # Adjust vocab_size and num_classes as needed\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "metrics = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    accuracy_train = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for texts_embedded, labels in tqdm(dataloader_train, desc=f\"Epoch {epoch}\"):\n",
    "        outputs = model(texts_embedded)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = outputs.argmax(1)\n",
    "        accuracy_train += torch.sum(preds == labels)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_dir, f\"{epoch}.pth\"))\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Calculate sensitivity and specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}, '\n",
    "          f'Accuracy: {accuracy_train/len(all_labels):.4f}, '\n",
    "          f'Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "    metrics.append([loss.item(), accuracy_train/len(all_labels), sensitivity, specificity])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e9393841559a6",
   "metadata": {},
   "source": [
    "*The following cell display the latest confusion matrix. Due to the sorting we do, spam is the positive class and ham the negative:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e4027399d4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset_train.labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899219244233ec18",
   "metadata": {},
   "source": [
    "**5. $\\star$ During training we will calculate a number of metrics, create a function to plot these:**\n",
    "- Look up Sensitivity and Specificity if you are in doubt about what they measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7445d346fcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics):\n",
    "    metrics = np.array(metrics)\n",
    "    metric_names = ['Loss', 'Accuracy', 'Sensitivity', 'Specificity']\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    # Create plots\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    #TODO: 12-15 lines of code\n",
    "    \n",
    "    plt.tight_layout()  # Adjust the layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc60d24138eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3febab4a12a408",
   "metadata": {},
   "source": [
    "**6. During training we saved the network after each epoch, load the network you want to test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7782b6de186da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_load = 9\n",
    "model.load_state_dict(torch.load(os.path.join(model_save_dir, f\"{epoch_load}.pth\"), weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f48187eb35cca",
   "metadata": {},
   "source": [
    "**7. Test the network using the loop below:**\n",
    "- How well does it perform, would you use this for spam detection in a real application?\n",
    "- Can you improve it?\n",
    "- The comparison between a word and character model is especially interesting if you consider what the data consists of, why?\n",
    "The data contains a LOT of spelling mistakes and a word gram model will see two words that are the same but mispelled as two unique words, whereas the character model will have many tokens that end up the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b1962bd1df761",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TextDataset('data/test_data.txt', vocab=dataset_train.vocab, embedding=dataset_train.embedding, minn=minn, maxn=maxn)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "accuracy_test = 0\n",
    "loss_test = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "for texts_embedded, labels in dataloader_test:\n",
    "    outputs = model(texts_embedded)\n",
    "    loss_test += criterion(outputs, labels) \n",
    "    \n",
    "    preds = outputs.argmax(1)\n",
    "    accuracy_test += torch.sum(preds == labels)\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "tp, fn, fp, tn = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Test loss: {loss_test.item():.4f}, Test accuracy: {accuracy_test / len(all_labels):.4f} \"\n",
    "      f'Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset_test.labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2daa357c084131",
   "metadata": {},
   "source": [
    "## Exercise 6: The Fasttext Library\n",
    "\n",
    "The above implementation of a Fasttext model is slow and not particularly accurate (unless you made some magic happen). In this exercise we will see how much more efficient and accurate the library implementation of Fasttext is.\n",
    "\n",
    "There are a number of parameters that can be passed to the FastText `train_supervised` function, but we will just concern ourselves with a couples of them.\n",
    "\n",
    "*The `input` parameter requires a text file as an input containing two columns. The first column must be the classification label and the second must be the text. The format we implemented above*\n",
    "*The `verbose` parameter just allows us to enable or disable training information. Here we enable it.*\n",
    "\n",
    "**1. Train a Fasttext model, this can be done with a single line of code**\n",
    "\n",
    "*How long did it take compared to the Pytorch implementation?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc0cae96cf4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_word_model = fasttext.train_supervised(input='./data/train_data.txt', verbose=True, wordNgrams=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fda955ed3d803c",
   "metadata": {},
   "source": [
    "**2. Implement the following test function that computes the same metrics as we did for our ownd function.**\n",
    "1. Loop across the test_texts and labels and use the model to predict the label\n",
    "    - `model.predict(text)[0][0]` returns a string with the signature `__label__{label}`\n",
    "2. Keep track of the predictions and labels in two separate lists\n",
    "3. Use the same function to calculate and display the confusion matrix\n",
    "4. Calculate the specificity and sensitivity using the same formulas as we did for the other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c29d7e69d66b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fasttext_model(test_texts, test_labels, fasttext_model, verbose=False):\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for text, label in zip(test_texts, test_labels):\n",
    "        #TODO: 3 lines of code\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    labels = sorted(set(test_labels))[::-1]  # Ensure consistent label order\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=labels)\n",
    "\n",
    "\n",
    "    # Calculate accuracy, sensitivity and specificity\n",
    "    #TODO: 4 lines of code\n",
    "    \n",
    "    # Print metrics if verbose\n",
    "    if verbose:\n",
    "        # Display confusion matrix\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "    \n",
    "        print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "        print(f\"Specificity: {specificity:.2f}\")\n",
    "        print(f'Model accuracy: {accuracy * 100:.2f} %')\n",
    "    \n",
    "    return accuracy, cm, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3a26e48519267",
   "metadata": {},
   "source": [
    "*Now test the model. We re-train here so it is easier to change the parameters and see the results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e63e9ecadb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_word_model = fasttext.train_supervised(input='./data/train_data.txt', verbose=True, wordNgrams=5)\n",
    "accuracy_word_model, cm_word_model, sens_word_model, spec_word_model = test_fasttext_model(test_texts, test_labels, fasttext_model=fasttext_word_model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de108a597b6a6a",
   "metadata": {},
   "source": [
    "**3. What happens when you vary the N-gram size (when testing)? What is the optimal setting? Why do you think that is the case?**\n",
    "    \n",
    "The lower the N the better. The dataset contains short sentences and few repeats of the same phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516b5da6cdf5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for N in range(1, 8):\n",
    "    fasttext_word_model = fasttext.train_supervised(input='./data/train_data.txt', verbose=True, wordNgrams=N)\n",
    "    accuracy_word_model, cm_word_model, sens_word_model, spec_word_model = test_fasttext_model(test_texts, test_labels, fasttext_model=fasttext_word_model, verbose=False)\n",
    "    metrics.append((N, accuracy_word_model, sens_word_model, spec_word_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e29deb1bb926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values, accuracies, sensitivities, specificities = zip(*metrics)\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_values, accuracies, marker='o', label=\"Accuracy\")\n",
    "plt.plot(n_values, sensitivities, marker='o', label=\"Sensitivity (Spam)\")\n",
    "plt.plot(n_values, specificities, marker='o', label=\"Specificity (Ham)\")\n",
    "\n",
    "# Labeling\n",
    "plt.title(\"Metrics vs. Word N-grams in FastText Model\")\n",
    "plt.xlabel(\"N (Word N-grams)\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.ylim(0, 1.1)  # Since metrics are between 0 and 1\n",
    "plt.xticks(n_values)  # Show only integer N values on the x-axis\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0992ab33ad9f169",
   "metadata": {},
   "source": [
    "**4. Look in the FastText documentation to find out how to make a character level model. Can you get better performance this way? Why do you think that is/isn't?**\n",
    "\n",
    "*HINT: Look at the `maxn` and `minn` parameters.*\n",
    "- Can you find optimal values for the min and max by performing a grid search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create char model here.\n",
    "char_gram_length_min = 1 # If set to zero, we only train word-grams\n",
    "char_gram_length_max = 3 # If set to zero, we only train word-grams\n",
    "\n",
    "fasttext_char_model = fasttext.train_supervised(\n",
    "    input='./data/train_data.txt',\n",
    "    verbose=True,\n",
    "    ?\n",
    ")\n",
    "accuracy_char_model, cm_char_model, sens_char_model, spec_char_model = test_fasttext_model(test_texts, test_labels, fasttext_model=fasttext_char_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd58d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1\n",
    "stop = 8\n",
    "results = []\n",
    "\n",
    "#TODO: 12 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e279cb086a8da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(results, annot=True, cmap=\"viridis\", xticklabels=range(1, 8), yticklabels=range(1, 8))\n",
    "plt.xlabel(\"maxn (range parameter)\")\n",
    "plt.ylabel(\"minn (range parameter)\")\n",
    "plt.title(\"Heatmap of Accuracy for Different `minn` and `maxn` Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006f52cc63aa285",
   "metadata": {},
   "source": [
    "**5. With this new information about what minn and maxn to use, try running our homemade model with these parameters!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
