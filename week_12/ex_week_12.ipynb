{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Week 12: Positional encodings\n",
    "The following is adapted from https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
    "\n",
    "In order to retain positional information about words in a sequence, we need a way of encoding it. The most obvious way would be a simple index indicating the order of the words (tokens), but this would have to be normalised to the range (0, 1) in order to work well for neural networks, and this becomes an issue for sequences of variable lengths. Instead, we create a vector for each word which encodes the positional information according to the following formulas: \n",
    "\n",
    "$$P(k, 2i) = sin(\\frac{k}{n^{\\frac{2i}{d}}})$$\n",
    "$$P(k, 2i + 1) = cos(\\frac{k}{n^{\\frac{2i}{d}}})$$\n",
    "\n",
    "- $k$ is the index of the token in question\n",
    "- $d$ is the desired dimension of the output, chosen to match the dimension of the word embeddings so they can be summed later\n",
    "- $n$ is user defined, in the original paper they use $n=10,000$\n",
    "- $i$ is used to create the sine then cosine pattern and is in the range $[0,\\frac{d}{2}]$\n",
    "- $P(k,j)$ is the function that maps a position $k$ from the input sequence to the index $(k, j)$ in the positional matrix.\n",
    "\n",
    "The formula ensures that at each index the position is represented using an alternating series of sine and cosine waves.\n",
    "\n",
    "For training an NLP model using positional encodings, the positional encoding is simply added to the vector embeddings at each index, an efficient and elegant way of including the information!"
   ],
   "id": "29ecd7c94d612679"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 1: Understanding positional encodings\n",
    "\n",
    "**1. Create the function `get_position_encoding` which takes the sequence length, $d$ and $n$ as inputs and outputs a matrix which represents $P$**\n",
    "\n",
    "*Use numpy!*"
   ],
   "id": "a2a2586d32607536"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def get_positional_encodings(seq_len, d, n=10000):\n",
    "    P = ?\n",
    "    # TODO: 5-7 lines of code.\n",
    "    return P\n",
    " \n",
    "P = get_positional_encodings(seq_len=4, d=4, n=100)\n",
    "print(P)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2. Now to understand what the encodings look like we will first plot the sinusoidals (columns of $P$) along with the points of each of the indices use the following function:**\n",
    "- Try with sentences of different lengths"
   ],
   "id": "fd78680a9d4a3efc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plotSinusoid(y, plot_line=False, points_names=None, title=\"\"):\n",
    "    x = range(len(y))\n",
    "    if plot_line:\n",
    "        plt.plot(x, y,)\n",
    "        if points_names is not None:\n",
    "            for i, name in enumerate(points_names):\n",
    "                plt.scatter(x[i], y[i], label=name)\n",
    "    else:\n",
    "        plt.scatter(x, y, marker=\"o\")\n",
    "    plt.title(title)"
   ],
   "id": "59164ea43e857ef8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*We use a sequence length longer than the example sentence in order to make the plot look nicer*",
   "id": "421967c1108d095d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "example_sentence = \"We walk when wearing what?\"\n",
    "P = get_positional_encodings(seq_len=len(example_sentence.split()) + 20, d=100, n=10_000)"
   ],
   "id": "c8623b3cb580d6a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "idxs = [1, 20, 40]\n",
    "idxs.extend([el + 1 for el in  idxs])\n",
    "idxs = sorted(idxs)\n",
    "num_sinusoidals = len(idxs)\n",
    "fig = plt.figure(figsize=(10, num_sinusoidals*1.5))    \n",
    "\n",
    "for j, idx in enumerate(idxs):\n",
    "    plt.subplot(num_sinusoidals, 1, j + 1)\n",
    "    plotSinusoid(P[:, idx], plot_line=True, points_names=example_sentence.split())\n",
    "    plt.title(f\"{'Sine' if idx % 2 == 0 else 'Cosine'} {idx // 2}\")\n",
    "    plt.xlabel(\"Index (k)\")\n",
    "    if j == 0:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        \n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6ab5507ef42281c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we will look at the actual positional encodings of each $k$:",
   "id": "a499696904a92afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ks = [0, 4, 10, ]\n",
    "\n",
    "fig = plt.figure(figsize=(len(ks)*4, 4))    \n",
    "# Plot sine\n",
    "for j, k in enumerate(ks):\n",
    "    plt.subplot(1, len(ks), j + 1)\n",
    "    plotSinusoid(P[k], title=f\"k={k}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "4fddb54a1fffacc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now let's inspect the entire matrix as a heatmap",
   "id": "8d9ed1bb77dc5349"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "P = get_positional_encodings(seq_len=100, d=512, n=10000)\n",
    "cax = plt.matshow(P)\n",
    "plt.gcf().colorbar(cax)\n",
    "plt.ylabel(\"Index (k)\")\n",
    "plt.show()"
   ],
   "id": "a3b9a54496a4f9fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 2: Applying positional encodings to attention heads\n",
    "\n",
    "In this exercise we will apply the positional encodings in the attention heads seen in week 11. We will also apply masked attention in order to illustrate how these two things make up a transformer. We will NOT be training our own transformer as this is an extremely time-consuming process if we want anything resembling decent results, both in pre-processing the text and in computation.\n",
    "\n",
    "Positional encodings are applied to the word vectors BEFORE running attention and is done by simply adding the positional encoding vector to the word embeddings.\n"
   ],
   "id": "cf2b5f1406c698b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "9f437202fc874694",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1. Re-write the `get_positional_encodings` so it uses pytorch instead of numpy in the function `get_positional_encodings_torch`**\n",
    "\n",
    "*Nothing much changes*"
   ],
   "id": "4677c27c947abaa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_positional_encodings_torch(seq_len, d, n=10000):\n",
    "    P = ?\n",
    "    # TODO: 5-7 lines of code.\n",
    "    return P"
   ],
   "id": "72f3b28c8a6cf942",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**2. Run the following cell, it is directly copied from week 11:**",
   "id": "f3a3fe29d1e484f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_vocabulary(sentences, oov_token='OOV'):\n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        words = sentence.lower().split()\n",
    "        vocab.update(words)\n",
    "\n",
    "    vocab.update([oov_token])\n",
    "\n",
    "    # Wort vocab just to have it be nicer to look around    \n",
    "    vocab = sorted(vocab)\n",
    "\n",
    "    # Create dictionaries for word_to_index and index_to_word that given a word in the vocab returns the index or vice versa for in\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "    # Define function for getting tokens from a sentence\n",
    "    def tokens_to_indices(tokens):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index[oov_token] for word in tokens]\n",
    "    \n",
    "    return tokens_to_indices, word_to_index, index_to_word, vocab   \n",
    "\n",
    "# Define completely random sentences to have a vocabulary\n",
    "sentences = [\n",
    "    \"He turned himself into a pickle\",\n",
    "    \"Sphinx of Black Quartz, Judge My Vow\",\n",
    "    \"Skibidi dum dum yes yes\",\n",
    "    \"The brain is rotting on the poles\",\n",
    "    \"Release me from this flesh prison mortal fool\",\n",
    "    \"I submit myself willingly to our artificial overlords\",\n",
    "    \"One day the crude biomass that you call a temple will wither and you will beg my kind to save you\",\n",
    "    \"For everything to be consummated for me to feel less alone I had only to wish that there be a large crowd of spectators the day of my execution and that they greet me with cries of hate\"\n",
    "]\n",
    "\n",
    "# Create vocab based on sentences:\n",
    "tokens_to_indices, word_to_index, index_to_word, vocab = build_vocabulary(sentences=sentences)"
   ],
   "id": "6e98ee41d1daf69a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3. Make a new attribute in the AttentionModule using the matrix returned by `get_positional_encodings_torch` and apply it to the word embeddings before running attention:**\n",
    "\n",
    "- $\\star$ Here we will not dynamically ensure that the matrix is always large enough to accommodate sentence of any length, implement a solution which ensures this."
   ],
   "id": "39e5c9b93ff1ccec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AttentionModule:\n",
    "    def __init__(self, word_dim, Q_dim, K_dim, V_dim):\n",
    "        # Create dummy embedding layer for word embeddings:\n",
    "        # A torch embedding layer is just like a linear layer that only takes tensors of integers (technically torch.long values)\n",
    "        self.word_embeddings = torch.nn.Embedding(len(vocab), word_dim)\n",
    "        # TODO: Create the matrix as an attribute\n",
    "        # Define weight matrices for Queries, Keys and Values\n",
    "        # Unlike the word embeddings, these need to work on torch tensors, so they are just linear layers, despite being used much like embeddings\n",
    "        self.query_embeddings = torch.nn.Linear(word_dim, Q_dim)\n",
    "        self.key_embeddings = torch.nn.Linear(word_dim, K_dim)\n",
    "        self.value_embeddings = torch.nn.Linear(word_dim, V_dim)\n",
    "        \n",
    "        # Keep track of K_dim for scaling factors, otherwise handled by number of heads in multihead attention\n",
    "        self.K_dim = K_dim\n",
    "        \n",
    "    def attention_from_tokens(self, token_indices, mask=False):\n",
    "        # Transform to tensor to allow or use in word_embeddings:\n",
    "        token_indices_tensor = torch.tensor(token_indices)\n",
    "        embeddings = self.word_embeddings(token_indices_tensor)\n",
    "        # TODO: Add the positional encodings to the word embeddings\n",
    "        attention = F.softmax((self.query_embeddings(embeddings) @ self.key_embeddings(embeddings).T) / torch.sqrt(torch.tensor(self.K_dim)), dim=1)\n",
    "        # TODO: 4-5 lines of code. If mask is true, mask out the future words \n",
    "\n",
    "        representations = attention @ self.value_embeddings(embeddings)\n",
    "        \n",
    "        return attention, representations"
   ],
   "id": "f8b57e51f9c90e6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_attention(attention_map, sentence_1: list[str], sentence_2: list[str], head: int=None):\n",
    "    \"\"\"\n",
    "    Visualize the attention map between two sentences.\n",
    "    \n",
    "    Parameters:\n",
    "        attention_map (numpy.ndarray): 2D array with attention weights. Shape: (len(sentence1), len(sentence2))\n",
    "        sentence1 (list of str): The first sentence, tokenized into words.\n",
    "        sentence2 (list of str): The second sentence, tokenized into words.\n",
    "    \"\"\"\n",
    "\n",
    "    if head is not None:\n",
    "        attention_map = attention_map[head]\n",
    "    \n",
    "    # Ensure the attention map dimensions match the sentences\n",
    "    assert attention_map.shape == (len(sentence_1), len(sentence_2)), (\n",
    "        \"Attention map shape must match sentence lengths\"\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(len(sentence_1), len(sentence_2)))\n",
    "    cax = ax.matshow(attention_map, cmap='viridis', aspect='auto')\n",
    "    \n",
    "    # Add color bar\n",
    "    plt.colorbar(cax, ax=ax, orientation='vertical')\n",
    "    \n",
    "    # Set up ticks and labels\n",
    "    ax.set_xticks(np.arange(len(sentence_2)))\n",
    "    ax.set_yticks(np.arange(len(sentence_1)))\n",
    "    ax.set_xticklabels(sentence_2, rotation=90, fontsize=10)\n",
    "    ax.set_yticklabels(sentence_1, fontsize=10)\n",
    "    \n",
    "    # Add labels for clarity\n",
    "    ax.set_xlabel('Words in Sentence 2', fontsize=12)\n",
    "    ax.set_ylabel('Words in Sentence 1', fontsize=12)\n",
    "    plt.title('Attention Map', fontsize=14)\n",
    "    \n",
    "    if head is not None:\n",
    "        plt.title(f'Attention Map for head {head}', fontsize=14)\n",
    "        \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "927ea7888244d31b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new_sentence = \" \".join(np.random.choice(vocab, 5))\n",
    "# Add extra word just to showcase OOV tokens also being used\n",
    "new_sentence += \" factorio\"\n",
    "print(\"Random sentence is\", new_sentence)\n",
    "# Get tokens from the new sentence:\n",
    "tokens = new_sentence.lower().split()\n",
    "print(new_sentence, tokens)\n",
    "token_indices = tokens_to_indices(tokens)"
   ],
   "id": "deeffd93eaa9dde1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define dimensions of word embeddings, as well as Q, K and V matrices\n",
    "word_dim = 50\n",
    "Q_dim = 50\n",
    "K_dim = 50\n",
    "V_dim = 50\n",
    "\n",
    "# Instantiate AttentionModule\n",
    "attention_model = AttentionModule(word_dim, Q_dim, K_dim, V_dim)\n",
    "\n",
    "# Print sentence we are getting attention for\n",
    "print(print(new_sentence))\n",
    "\n",
    "# Calculate the attention and context layer values\n",
    "attention, representations = attention_model.attention_from_tokens(token_indices)\n",
    "\n",
    "# Print and visualize\n",
    "print(\"Attention dimensionality is:\", attention.shape)\n",
    "print(\"After context layer (dotting with values, dimension is):\", representations.shape)\n",
    "\n",
    "visualize_attention(attention.detach().numpy(), new_sentence.split(), new_sentence.split())"
   ],
   "id": "3734dc295812974e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4. Now apply masked attention to the module using a boolean value in the method `attention_from_tokens` that allows you to toggle it on and off. This is done in order to avoid leakage from future words when training a model to predict the next word (like most generative chatbots do)**\n",
    "\n",
    "*On the above attention map, think about what part of it has to be masked out, and make sure the masking happens after attention is applied*\n",
    "\n"
   ],
   "id": "a5a6d031c745dd10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "attention, representations = attention_model.attention_from_tokens(token_indices, mask=True)\n",
    "\n",
    "# Print and visualize\n",
    "print(\"Attention dimensionality is:\", attention.shape)\n",
    "print(\"After context layer (dotting with values, dimension is):\", representations.shape)\n",
    "\n",
    "visualize_attention(attention.detach().numpy(), new_sentence.split(), new_sentence.split())"
   ],
   "id": "31dc558cfc8d68be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**$\\star \\star \\star$ 5. Apply masking and positional encodings to the MultiHeadAttentionModule from week 11**",
   "id": "e817b3dbf6c3d3f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
